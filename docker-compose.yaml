version: '3.9'
services:
  llama:
    image: ghcr.io/go-skynet/llama-cpp-server:latest
    volumes:
      - ./models:/models
    ports:
      - "8080:8080"
    command: ["--model", "/models/your-model.gguf"]

  codex:
    build: .
    depends_on:
      - llama
    ports:
      - "8081:8081"
    volumes:
      - ./data:/data