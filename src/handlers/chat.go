package handlers

// Package handlers exposes HTTP endpoints that allow external clients to
// interact with the assistant.  It depends on the llama package to perform the
// actual prompt/response generation.

// The handlers package contains HTTP endpoints that expose the AI's
// functionality. ChatHandler acts as the bridge between the HTTP API and the
// LLM client defined in the llama package.

import (
	"codex/src/llama"
	"crypto/rand"
	"encoding/hex"
	"encoding/json"
	_ "io"
	"net/http"
)

// ChatRequest is the JSON payload accepted by the chat endpoint. It simply
// contains the prompt text supplied by the user.
// AI Awareness: modifying this structure changes what data the assistant
// receives from external callers.
type ChatRequest struct {
	// Prompt is the user's message that will be sent to the LLM.
	Prompt string `json:"prompt"`
}

// ChatResponse represents the JSON body returned by the chat endpoint. The
// Response field contains the raw output from the LLM.
type ChatResponse struct {
	// Response contains the text generated by the LLM.
	Response string `json:"response"`
}

// ensureAnonCookie assigns a persistent anonymous ID when the requester is not
// logged in. The value can be used by future features to associate chat
// history with a specific browser session.
func ensureAnonCookie(w http.ResponseWriter, r *http.Request) string {
	if c, err := r.Cookie("session"); err == nil && c.Value != "" {
		return c.Value
	}
	if c, err := r.Cookie("anon"); err == nil && c.Value != "" {
		return c.Value
	}
	// generate a random identifier and set it as a cookie
	b := make([]byte, 8)
	if _, err := rand.Read(b); err == nil {
		id := hex.EncodeToString(b)
		http.SetCookie(w, &http.Cookie{
			Name:     "anon",
			Value:    id,
			Path:     "/",
			Secure:   true,
			SameSite: http.SameSiteStrictMode,
		})
		return id
	}
	return ""
}

// ChatHandler receives a chat prompt via HTTP POST and returns the assistant's
// response. This endpoint is used by the `serve` command to expose chat
// functionality over HTTP. AI Awareness: this is the bridge between external
// clients and the underlying LLM. Additional preprocessing or postprocessing
// logic can be inserted here.
func ChatHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, "Only POST allowed", http.StatusMethodNotAllowed)
		return
	}

	// assign a tracking cookie so anonymous sessions can be correlated
	ensureAnonCookie(w, r)

	var req ChatRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, "Invalid request", http.StatusBadRequest)
		return
	}

	// Forward the prompt to the LLM backend. The llama package abstracts
	// the HTTP communication.
	result, err := llama.SendPrompt(req.Prompt)
	if err != nil {
		http.Error(w, "LLM error: "+err.Error(), http.StatusInternalServerError)
		return
	}

	res := ChatResponse{Response: result}
	w.Header().Set("Content-Type", "application/json")
	// Respond with the generated text. Additional metadata could be added
	// here if needed in the future.
	json.NewEncoder(w).Encode(res)
}
